{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This post uses the [New York Times API](http://developer.nytimes.com/docs/read/article_search_api_v2) to search for articles on US politics that include the word *scandal*, and several python libraries to grab the text of those articles and store them to MongoDB for some natural language processing analytics (in [part 2](|filename|/nyt-nlp.ipynb) of this project)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These commands should install some of the dependencies for this part of the project:\n",
      "\n",
      "    pip install pymongo\n",
      "    pip install requests\n",
      "    pip install lxml\n",
      "    pip install cssselect"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "from time import sleep\n",
      "import itertools\n",
      "import functools\n",
      "from lxml.cssselect import CSSSelector\n",
      "from lxml.html import fromstring\n",
      "import pymongo\n",
      "import datetime as dt\n",
      "from operator import itemgetter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autosave\n",
      "%autosave 30"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: %autosave [seconds]\n",
        "autosaving every 30s\n"
       ]
      },
      {
       "javascript": [
        "\n",
        "\n",
        "// clear previous interval, if there was one\n",
        "if (IPython.autosave_extension_interval) {\n",
        "    clearInterval(IPython.autosave_extension_interval);\n",
        "    IPython.autosave_extension_interval = null;\n",
        "}\n",
        "\n",
        "// set new interval\n",
        "if (30000) {\n",
        "    console.log(\"scheduling autosave every 30000 ms\");\n",
        "    IPython.notebook.save_notebook();\n",
        "    IPython.autosave_extension_interval = setInterval(function() {\n",
        "        console.log(\"autosave\");\n",
        "        IPython.notebook.save_notebook();\n",
        "    }, 30000);\n",
        "} else {\n",
        "    console.log(\"canceling autosave\");\n",
        "}\n"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x10455e110>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Mongodb"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to connect to the database, assuming it's already running (`mongod` from the terminal)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "connection = pymongo.Connection(\"localhost\", 27017 )\n",
      "db = connection.nyt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get URLs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After using your secret API key..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from key import apikey\n",
      "apiparams = {'api-key': apikey}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...the first thing we need to get is the urls for all the articles that match our search criterion. [Big caveat: I used a v1 api query for the original dataset that I used, and modified it for v2 after discovering it].\n",
      "\n",
      "I searched for *scandal* with the `q` parameter, and narrowed it down using *republican OR democrat* with the `f[ilter]q[uery]` parameter. I found out that there are lots of really interesting curated details you can use in the search, such as searching for articles pertaining to certain geographic areas, people or organizations (with a feature called *facets*. I used other parameters to restrict the dates to years 1992-2013, and just return certain fields I thought would be relevant:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "page = 0\n",
      "\n",
      "q = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
      "params = {'q': 'scandal*',\n",
      "            'fq': 'republican* OR democrat*',\n",
      "            'fl': 'web_url,headline,pub_date,type_of_material,document_type,news_desk',\n",
      "            'begin_date': '19920101',\n",
      "            'end_date': '20131231',\n",
      "            'page': page,\n",
      "            'api-key': apikey,\n",
      "            'rank': 'newest',\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After constructing the query, we grab the search results with [requests](http://docs.python-requests.org/en/latest/). There's no way to tell how many results there will be, so we go as long as we can, shoving everything into MongoDB, incrementing the `offset` query parameter and pausing for a break before the next page of results (the NYT has a limit on how many times you can query them per second)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def memoize(f):\n",
      "    \"Memoization for args and kwargs\"\n",
      "    @functools.wraps(f)\n",
      "    def wrapper(*args, **kwargs):\n",
      "        kw_tup = tuple((kargname, tuple(sorted(karg.items()))) for kargname, karg in kwargs.items())\n",
      "        memo_args = args + kw_tup\n",
      "        try:\n",
      "            return wrapper.cache[memo_args]\n",
      "        except KeyError:\n",
      "            print '*',\n",
      "            wrapper.cache[memo_args] = res = f(*args, **kwargs)\n",
      "            return res\n",
      "    wrapper.cache = {}\n",
      "    return wrapper\n",
      "\n",
      "    \n",
      "@memoize\n",
      "def search_nyt(query, params={}):\n",
      "    r = requests.get(query, params=params)\n",
      "    res = json.loads(r.content)\n",
      "    sleep(.1)\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdate = lambda d, fmt='%Y%m%d': dt.datetime.strptime(d, fmt)\n",
      "\n",
      "for page in itertools.count():  #keep looping indefinitely\n",
      "    params.update({'page': page})  #fetch another ten results from the next page\n",
      "    res = search_nyt(q, params=params)[\"response\"]\n",
      "    if res['docs']:\n",
      "        for dct in res['docs']:\n",
      "            dct = dct.copy()  #for memoization purposes\n",
      "            url = dct.pop('web_url')\n",
      "            dct['pub_date'] = fdate(dct['pub_date'].split('T')[0], '%Y-%m-%d')  #string -> format as datetime object\n",
      "            dct['headline'] = dct['headline']['main']\n",
      "            db.raw_text.update({'url': url}, {'$set': dct}, upsert=True)\n",
      "    else:  #no more results\n",
      "        break\n",
      "    if page % 100 == 0:\n",
      "        print page,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see from the response's metadata that we should expect about 11876 article links to be saved to our database:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_nyt(q, params=params)['response']['meta']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{u'hits': 11876, u'offset': 340, u'time': 140}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Scrape Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are a few of the resulting URLS that we'll use to get the full text articles:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[doc['url'] for doc in db.raw_text.find()][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'http://www.nytimes.com/2006/04/30/washington/30cunningham.html',\n",
        " u'http://www.nytimes.com/2004/04/09/business/senate-panel-asked-to-give-sec-proposals-a-chance.html',\n",
        " u'http://www.nytimes.com/2006/05/12/washington/12foggo.html',\n",
        " u'http://cityroom.blogs.nytimes.com/2007/08/14/tributes-to-mrs-astor-federal-money-for-congestion-pricing-tribulations-of-newarks-mayor-and-more/',\n",
        " u'http://www.nytimes.com/2007/08/14/opinion/14tue1.html']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scraping wasn't as difficult as I was expecting; over the 20 or so years that I searched for, the body text of the articles could be found by looking at 5 html elements (formatted as CSS selectors in `_sels` below). The following two functions do most of the scraping work-- `get_text`...well...gets the text from the `CSSSelector` parser, and `grab_text` uses this after pulling the html with requests."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(e):\n",
      "    \"Function to extract text from CSSSelector results\"\n",
      "    try:\n",
      "        return ' '.join(e.itertext()).strip().encode('ascii', 'ignore')\n",
      "    except UnicodeDecodeError:\n",
      "        return ''\n",
      "\n",
      "\n",
      "def grab_text(url, verbose=True):\n",
      "    \"Main scraping function--given url, grabs html, looks for and returns article text\"\n",
      "    if verbose and (grab_text.c % verbose == 0):  #page counter\n",
      "        print grab_text.c,\n",
      "    grab_text.c += 1\n",
      "    r = requests.get(url, params=all_pages)\n",
      "    content = fromstring(r.content)\n",
      "    for _sel in _sels:\n",
      "        text_elems = CSSSelector(_sel)(content)\n",
      "        if text_elems:\n",
      "            return '\\n'.join(map(get_text, text_elems))\n",
      "    return ''\n",
      "\n",
      "#Selectors where text of articles can be found; several patterns among NYT articles\n",
      "_sels =  ['p[itemprop=\"articleBody\"]', \"div.blurb-text\", 'div#articleBody p', 'div.articleBody p', 'div.mod-nytimesarticletext p']\n",
      "all_pages = {'pagewanted': 'all'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And here is the main loop and counter. Pretty simple.\n",
      "\n",
      "On the first run, the output counts up from zero, but since political scandals seem to be popping up by the hour, I've updated the search a few times, but only pulling articles that aren't already in the database (hence the very sparse output below)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exceptions = []\n",
      "grab_text.c = 0\n",
      "\n",
      "for doc in db.raw_text.find(timeout=False):\n",
      "    if ('url' in doc) and ('text' not in doc):\n",
      "        # if we don't already have this in mongodb\n",
      "        try:\n",
      "            txt = grab_text(doc['url'], verbose=1)        \n",
      "        except Exception, e:\n",
      "            exceptions.append((e, doc['url']))\n",
      "            print ':(',\n",
      "            continue\n",
      "        db.raw_text.update({'url': doc['url']}, {'$set': {'text': txt}})\n",
      "    else:\n",
      "        pass\n",
      "\n",
      "db.raw_text.remove({'text': u''})  #there was one weird result that didn't have any text..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":( 1 :( 2 :( 3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":( 4 :( "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 :( 6 :( 7 :( 8 :( 9 :( 10 :(\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I used the following code to clean up after I downloaded most of the articles and modified the query, which changed some of the output fields. This just renames the original fields and sets the `title` to an empty string where there was none. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for doc in db.raw_text.find(timeout=False):\n",
      "    if ('pub_date' in doc) and ('date' not in doc):\n",
      "        # cleanup from v1 queries\n",
      "        updates = {'date': doc['pub_date'],\n",
      "                   'title': doc['headline']}\n",
      "        db.raw_text.update({'url': doc['url']}, {'$set': updates})\n",
      "    if ('title' not in doc) and ('headline' not in doc):\n",
      "        #cleanup\n",
      "        db.raw_text.update({'url': doc['url']}, {'$set': {'title': ''}})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A few of the articles set off exceptions when I tried pulling them; these weren't included in the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exceptions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[(lxml.etree.ParserError('Document is empty'),\n",
        "  u'http://www.nytimes.com/2007/04/01/magazine/01axelrod.t.html'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2007/01/22/992348.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2007/01/22/992348.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2006/02/16/757594.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2006/02/16/757594.sgml'),\n",
        " (lxml.etree.ParserError('Document is empty'),\n",
        "  u'http://www.nytimes.com/2006/01/23/politics/23leases.html'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2005/11/08/732095.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2005/11/08/732095.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2009/08/25/954306.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2009/08/25/954306.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2009/07/30/131903.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2009/07/30/131903.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2009/01/22/881090.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2009/01/22/881090.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2008/12/23/615110.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2008/12/23/615110.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2006/12/12/830470.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2006/12/12/830470.sgml'),\n",
        " (requests.exceptions.MissingSchema(\"Invalid URL u'/data/daily/2005/05/21/030015.sgml': No schema supplied\"),\n",
        "  u'/data/daily/2005/05/21/030015.sgml')]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = map(itemgetter('date', 'url', 'title'), sorted(db.raw_text.find(), key=itemgetter('date'), reverse=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(datetime.datetime(2013, 7, 7, 0, 0),\n",
        "  u'http://www.nytimes.com/2013/07/07/business/mutfund/robert-ag-monks-crusading-against-corporate-excess.html',\n",
        "  u'Robert A.G. Monks, Crusading Against Corporate Excess'),\n",
        " (datetime.datetime(2013, 7, 6, 0, 0),\n",
        "  u'http://www.nytimes.com/reuters/2013/07/06/world/europe/06reuters-usa-security-europe-germany.html',\n",
        "  u'Merkel Says EU Must Not Forget U.S. Spying in Push for Free Trade'),\n",
        " (datetime.datetime(2013, 7, 6, 0, 0),\n",
        "  u'http://www.nytimes.com/2013/07/06/nyregion/weiners-surprising-rebound-from-scandal.html',\n",
        "  u'Weiner\\u2019s Surprising Rebound From Scandal')]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And, we got more than 9700 scandalous stories...but still counting!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(db.raw_text.find()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "9717"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Conclusion\n",
      "Though it turned out to be pretty brief, I thought this first part of my NYT scandals project deserved its own post.\n",
      "Luckily it doesn't take too much effort or space when you're working with a nice, expressive language, though.\n",
      "And you can reproduce this for yourself--you can find a copy of this notebook on [Github](https://github.com/d10genes/nyt-nlp)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
